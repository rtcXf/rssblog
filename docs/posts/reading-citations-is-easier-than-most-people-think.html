<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Reading citations is easier than most people think | Rahul Vishwakarma Blog</title>
    <link rel="stylesheet" type="text/css" href="../styles.css" media="screen" />
</head>
<body>
<header>
    <nav>
        <a href="/index.html" aria-label="Go back to the homepage">← Back</a>
        <a href="https://danluu.com/dunning-kruger/" target="_blank" rel="noopener noreferrer">
            View Original
        </a>
    </nav>
</header>

<main>
    <article>
        <h1>Reading citations is easier than most people think</h1>
        <section>
            
            <div id="readability-page-1" class="page"><div> <p>It&#39;s really common to see claims that some meme is backed by “studies” or “science”. But when I look at the actual studies, it usually turns out that the data are opposed to the claim. Here are the last few instances of this that I&#39;ve run across. </p> <h2 id="dunning-kruger">Dunning-Kruger</h2> <p>A pop-sci version of Dunning-Kruger, the most common one I see cited, is that, the less someone knows about a subject, the more they think they know. Another pop-sci version is that people who know little about something overestimate their expertise because their lack of knowledge fools them into thinking that they know more than they do. The actual claim Dunning and Kruger make is much weaker than the first pop-sci claim and, IMO, the evidence is weaker than the second claim. The original paper isn&#39;t much longer than most of the incorrect pop-sci treatments of the paper, and we can get pretty good idea of the claims by looking at the four figures included in the paper. In the graphs below, “perceived ability” is a subjective self rating, and “actual ability” is the result of a test.</p> <p><img src="https://danluu.com/images/dunning-kruger/dunning_1.png" alt="Dunning-Kruger graph" width="333" height="301"/> <img src="https://danluu.com/images/dunning-kruger/dunning_2.png" alt="Dunning-Kruger graph" width="334" height="302"/> <img src="https://danluu.com/images/dunning-kruger/dunning_3.png" alt="Dunning-Kruger graph" width="335" height="304"/> <img src="https://danluu.com/images/dunning-kruger/dunning_4.png" alt="Dunning-Kruger graph" width="331" height="299"/></p> <p>In two of the four cases, there&#39;s an obvious positive correlation between perceived skill and actual skill, which is the opposite of the first pop-sci conception of Dunning-Kruger that we discussed. As for the second, we can see that people at the top end also don&#39;t rate themselves correctly, so the explanation for Dunning-Kruger&#39;s results is that people who don&#39;t know much about a subject (an easy interpretation to have of the study, given its title, <cite>Unskilled and Unaware of It: How Difficulties in Recognizing One&#39;s Own Incompetence Lead to Inflated Self-Assessments</cite>) is insufficient because that doesn&#39;t explain why people at the top of the charts have what appears to be, at least under the conditions of the study, a symmetrically incorrect guess about their skill level. One could argue that there&#39;s a completely different effect that just happens to cause the same, roughly linear, slope in perceived ability that people who are &#34;unskilled and unaware of it&#34; have. But, if there&#39;s any plausible simpler explanation, then that explanation seems overly complicated without additional evidence (which, if any exists, is not provided in the paper).</p> <p>A plausible explanation of why perceived skill is compressed, especially at the low end, is that few people want to rate themselves as below average or as the absolute best, shrinking the scale but keeping a roughly linear fit. The crossing point of the scales is above the median, indicating that people, on average, overestimate themselves, but that&#39;s not surprising given the population tested (more on this later). In the other two cases, the correlation is very close to zero. It could be that the effect is different for different tasks, or it could be just that the sample size is small and that the differences between the different tasks is noise. It could also be that the effect comes from the specific population sampled (students at Cornell, who are probably actually above average in many respects). If you look up Dunning-Kruger on Wikipedia, it claims that a replication of Dunning-Kruger on East Asians shows the opposite result (perceived skill is lower than actual skill, and the greater the skill, the greater the difference), and that the effect is possibly just an artifact of American culture, but the citation is actually a link to an editorial which mentions a meta analysis on East Asian confidence, so that might be another example of a false citation. Or maybe it&#39;s just a link to the wrong source. In any case, the effect certainly isn&#39;t that the more people know, the less they think they know.</p> <h2 id="income-happiness">Income &amp; Happiness</h2> <p>It&#39;s become common knowledge that money doesn&#39;t make people happy. As of this writing, a Google search for <code>happiness income</code> returns a knowledge card that making more than $75k/year has no impact on happiness. Other top search results claim the happiness ceiling occurs at $10k/year, $30k/year, $40k/year and $75k/year.</p> <p><img src="https://danluu.com/images/dunning-kruger/google_happiness.png" alt="Google knowledge card says that $75k should be enough for anyone" width="603" height="479"/></p> <p>Not only is that wrong, the wrongness <a href="https://web.archive.org/web/20160610202901/http://www.brookings.edu/~/media/research/files/papers/2013/04/subjective%20well%20being%20income/subjective%20well%20being%20income.pdf">is robust across every country studied, too</a>.</p> <p><img src="https://danluu.com/images/dunning-kruger/happiness_within_country.png" alt="People with more income are happier" width="629" height="506"/></p> <p>That happiness is correlated with income doesn&#39;t come from cherry picking one study. That result holds across five iterations of the World Values Survey (1981-1984, 1989-1993, 1994-1999, 2000-2004, and 2005-2009), three iterations of the Pew Global Attitudes Survey (2002, 2007, 2010), five iterations of the International Social Survey Program (1991, 1998, 2001, 2007, 2008), and a large scale Gallup survey.</p> <p>The graph above has income on a log scale, if you pick a country and graph the results on a linear scale, you get <a href="https://gravityandlevity.wordpress.com/2013/05/06/how-we-measure-our-happiness/">something like this</a>.</p> <p><img src="https://danluu.com/images/dunning-kruger/happiness_income_linear.gif" alt="Best fit log to happiness vs. income" width="576" height="335"/></p> <p>As with all graphs of a log function, it looks like the graph is about to level off, which results in interpretations like the following:</p> <p><img src="https://danluu.com/images/dunning-kruger/bad_log_1.png" alt="Distorted log graph" width="291" height="393"/></p> <p>That&#39;s an actual graph from an article that claims that income doesn&#39;t make people happy. These vaguely log-like graphs that level off are really common. If you want to see more of these, try an <a href="https://duckduckgo.com/?q=happiness+income&amp;iax=1&amp;ia=images">image search for “happiness income”</a>. My favorite is the one where people who make enough money literally hit the top of the scale. Apparently, there&#39;s a dollar value which not only makes you happy, it makes you as happy as it is possible for humans to be.</p> <p>As with Dunning-Kruger, you can look at the graphs in the papers to see what&#39;s going on. It&#39;s a little easier to see why people would pass along the wrong story here, since it&#39;s easy to misinterpret the data when it&#39;s plotted against a linear scale, but it&#39;s still pretty easy to see what&#39;s going on by taking a peek at the actual studies.</p> <h2 id="hedonic-adaptation-happiness">Hedonic Adaptation &amp; Happiness</h2> <p>The idea that people bounce back from setbacks (as well as positive events) and return to a fixed level of happiness entered the popular consciousness after <a href="http://www.amazon.com/gp/product/1400077427/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1400077427&amp;linkCode=as2&amp;tag=abroaview-20&amp;linkId=3ERCGEMU57L2FDF7">Daniel Gilbert wrote about it in a popular book</a>.</p> <p>But even without looking at the literature on adaptation to adverse events, the previous section on wealth should cast some doubt on this. If people rebound from both bad events and good, how is it that making more money causes people to be happier?</p> <p>Turns out, the idea that people adapt to negative events and return to their previous set-point is a myth. Although the exact effects vary depending on the bad event, disability, divorce, loss of a partner, and unemployment all have long-term negative effects on happiness. Unemployment is the one event that can be undone relatively easily, but the effects persist even after people become reemployed. I&#39;m only citing four studies here, but <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3289759/">a meta analysis of the literature</a> shows that the results are robust across existing studies.</p> <p>The same thing applies to positive events. While it&#39;s “common knowledge” that winning the lottery doesn&#39;t make people happier, <a href="http://www.nytimes.com/2014/05/27/science/how-to-win-the-lottery-happily.html">it turns out that isn&#39;t true, either</a>.</p> <p>In both cases, early cross-sectional results indicated that it&#39;s plausible that extreme events, like winning the lottery or becoming disabled, don&#39;t have long term effects on happiness. But the longitudinal studies that follow individuals and measure the happiness of the same person over time as events happen show the opposite result -- events do, in fact, affect happiness. For the most part, these aren&#39;t new results (some of the initial results predate Daniel Gilbert&#39;s book), but the older results based on less rigorous studies continue to propagate faster than the corrections.</p> <h2 id="chess-position-memorization">Chess position memorization</h2> <p>I frequently see citations claiming that, while experts can memorize chess positions better than non-experts, the advantage completely goes away when positions are randomized. When people refer to a specific citation, it&#39;s generally Chase and Simon&#39;s 1973 paper Perception in Chess, a &#34;classic&#34; which has been cited a whopping 7449 times in the literature, which says:</p> <blockquote> <p>De Groat did, however, find an intriguing difference between masters and weaker players in his short-term memory experiments. Masters showed a remarkable ability to reconstruct a chess position almost perfectly after viewing it for only 5 sec. There was a sharp dropoff in this ability for players below the master level. This result could not be attributed to the masters’ generally superior memory ability, for when chess positions were constructed by placing the same numbers of pieces randomly on the board, the masters could then do no better in reconstructing them than weaker players, Hence, the masters appear to be constrained by the same severe short-term memory limits as everyone else ( Miller, 1956), and their superior performance with &#34;meaningful&#34; positions must lie in their ability to perceive structure in such positions and encode them in chunks. Specifically, if a chess master can remember the location of 20 or more pieces on the board, but has space for only about five chunks in short-term memory, then each chunk must be composed of four or five pieces, organized in a single relational structure.</p> </blockquote> <p>The paper then runs an experiment which &#34;proves&#34; that master-level players actually do worse than beginners when memorizing random mid-game positions even though they do much better memorizing real mid-game positions (and, in end-game positions, they do the about the same as beginners when positions are randomized). Unfortunately, the paper used an absurdly small sample size of one chess player at each skill level.</p> <p>A quick search indicates that this result does not reproduce with larger sample sizes, e.g., Gobet and Simon, in &#34;Recall of rapidly presented random chess positions is a function of skill&#34;, say</p> <blockquote> <p>A widely cited result asserts that experts’ superiority over novices in recalling meaningful material from their domain of expertise vanishes when they are confronted with random material. A review of recent chess experiments in which random positions served as control material (presentation time between 3 and 10 sec) shows, however, that strong players generally maintain some superiority over weak players even with random positions, although the relative difference between skill levels is much smaller than with game positions. The implications of this finding for expertise in chess are discussed and the question of the recall of random material in other domains is raised.</p> </blockquote> <p>They find this scales with skill level and, e.g., for &#34;real&#34; positions, 2350+ ELO players memorized ~2.2x the number of correct pieces that 1600-2000 ELO players did, but the difference was ~1.6x for random positions (these ratios are from eyeballing a graph and may be a bit off). 1.6x is smaller than 2.2x, but it&#39;s certainly not the claimed 1.0.</p> <p>I&#39;ve also seen this result cited to claim that it applies to other fields, but in a quick search of applying this result to other fields, results either show something similar (a smaller but still observable difference on randomized positions) or don&#39;t reproduce, e.g., McKeithen did this for programmers and found that, on trying to memorize programs, on &#34;normal&#34; program experts were ~2.5x better than beginners on the first trial and 3x better by the 6th trial, whereas on the &#34;scrambled&#34; program, experts were 3x better on the first trial and progressed to being only ~1.5x better by the 6th trial. Despite this result contradicting Chase and Simon, I&#39;ve seen people cite this result to claim the same thing as Chase and Simon, presumably from people who didn&#39;t read what McKeithen actually wrote.</p> <h2 id="type-systems">Type Systems</h2> <p>Unfortunately, false claims about studies and evidence aren&#39;t limited to pop-sci memes; they&#39;re everywhere in both software and hardware development. For example, see this comment from a Scala/FP &#34;thought leader&#34;:</p> <p><img src="https://danluu.com/images/empirical-pl/pl_godwin.png" alt="Tweet claiming that any doubt that type systems are helpful is equivalent to being an anti-vaxxer" width="524" height="214"/></p> <p>I see something like this at least once a week. I&#39;m picking this example not because it&#39;s particularly egregious, but because it&#39;s typical. If you follow a few of the big time FP proponents on twitter, you&#39;ll see regularly claims that there&#39;s very strong empirical evidence and extensive studies backing up the effectiveness of type systems.</p> <p>However, <a href="https://danluu.com/empirical-pl/">a review of the empirical evidence</a> shows that the evidence is mostly incomplete, and that it&#39;s equivocal where it&#39;s not incomplete. Of all the false memes, I find this one to be the hardest to understand. In the other cases, I can see a plausible mechanism by which results could be misinterpreted. “Relationship is weaker than expected” can turn into “relationship is opposite of expected”, log can look a lot like an asymptotic function, and preliminary results using inferior methods can spread faster than better conducted follow-up studies. But I&#39;m not sure what the connection between the evidence and beliefs are in this case.</p> <h2 id="is-this-preventable">Is this preventable?</h2> <p>I can see why false memes might spread quickly, even when they directly contradict reliable sources. Reading papers sounds like a lot of work. It sometimes is. But it&#39;s often not. Reading a pure math paper is usually a lot of work. Reading an empirical paper to determine if the methodology is sound can be a lot of work. For example, biostatistics and econometrics papers tend to apply completely different methods, and it&#39;s a lot of work to get familiar enough with the set of methods used in any particular field to understand precisely when they&#39;re applicable and what holes they have. But reading empirical papers just to see what claims they make is usually pretty easy.</p> <p>If you read the abstract and conclusion, and then skim the paper for interesting bits (graphs, tables, telling flaws in the methodology, etc.), that&#39;s enough to see if popular claims about the paper are true in most cases. In my ideal world, you could get that out of just reading the abstract, but it&#39;s not uncommon for papers to make claims in the abstract that are much stronger than the claims made in the body of the paper, so you need to at least skim the paper.</p> <p>Maybe I&#39;m being naive here, but I think a major reason behind false memes is that checking sources sounds much harder and more intimidating than it actually is. A striking example of this is when Quartz published its article on how there isn&#39;t a gender gap in tech salaries, <a href="https://danluu.com/gender-gap/">which cited multiple sources that showed the exact opposite</a>. Twitter was abuzz with people proclaiming that the gender gap has disappeared. When I published <a href="https://danluu.com/gender-gap/">a post which did nothing but quote the actual cited studies</a>, many of the same people then proclaimed that their original proclamation was mistaken. It&#39;s great that they were willing to tweet a correction, but as far as I can tell no one actually went and read the source data, even though the graphs and tables make it immediately obvious that the author of the original Quartz article was pushing an agenda, not even with cherry picked citations, but citations that showed the opposite of their thesis.</p> <p>Unfortunately, it&#39;s in the best interests of non-altruistic people who do read studies to make it seem like reading studies is difficult. For example, when I talked to the founder of a widely used pay-walled site that reviews evidence on supplements and nutrition, he claimed that it was ridiculous to think that &#34;normal people&#34; could interpret studies correctly and that experts are needed to read and summarize studies for the masses. But he&#39;s just a serial entrepreneur who realized that you can make a lot of money by reading studies and summarizing the results! A more general example is how people sometimes try to maintain an authoritative air by saying that you need certain credentials or markers of prestige to really read or interpret studies.</p> <p>There are certainly fields where you need some background to properly interpret a study, but even then, the amount of knowledge that a degree contains is quite small and can be picked up by anyone. For example, excluding lab work (none of which contained critical knowledge for interpreting results), I was within a small constact factor of spending one hour of time per credit hour in school. At the conversion rate, an engineering degree from my alma mater costs a bit more than 100 hours and almost all non-engineering degrees land at less than 40 hours, with a large amount of overlap between them because a lot of degrees will require the same classes (e.g., calculus). Gatekeeping reading and interpreting a study on whether or not someone has a credential like a degree is absurd when someone can spend a week&#39;s worth of time gaining the knowledge that a degree offers.</p> <p><strong>If you liked this post, you&#39;ll probably enjoy <a href="https://danluu.com/discontinuities/">this post on odd discontinuities</a>, <a href="https://danluu.com/tech-discrimination/">this post how the effect of markets on discrimination is more nuanced than it&#39;s usually made out to be</a> and <a href="https://danluu.com/percentile-latency/">this other post discussing some common misconceptions</a>.</strong></p> <h3 id="2021-update">2021 update</h3> <p>In retrospect, I think the mystery of the &#34;type systems&#34; example is simple: it&#39;s a different kind of fake citation than the others. In the first three examples, a clever, contrarian, but actually wrong idea got passed around. This makes sense because people love clever, contrarian, ideas and don&#39;t care very much if they&#39;re wrong, so clever, contarian, relatively frequently become viral relative to their correctness.</p> <p>For the type systems example, it&#39;s just that people commonly fabricate evidence and then appeal to authority to support their position. In the post, I was confused because I couldn&#39;t see how anyone could look at the evidence and then make the claims that type system advocates do but, after reading thousands of discussions from people advocating for their pet tool/language/practice, I can see that it was naive of me to think that these advocates would even consider looking for evidence as opposed to just pretending that evidence exists without ever having looked.</p> <p><small> Thanks to Leah Hanson, Lindsey Kuper, Jay Weisskopf, Joe Wilder, Scott Feeney, Noah Ennis, Myk Pono, Heath Borders, Nate Clark, and Mateusz Konieczny for comments/corrections/discussion.</small></p> <p>BTW, if you&#39;re going to send me a note to tell me that I&#39;m obviously wrong, please make sure that I&#39;m actually wrong. In general, I get great feedback and I&#39;ve learned a lot from the feedback that I&#39;ve gotten, but the feedback I&#39;ve gotten on this post has been unusually poor. Many people have suggested that the studies I&#39;ve referenced have been debunked by some other study I clearly haven&#39;t read, but in every case so far, I&#39;ve already read the other study. </p>   </div></div>
        </section>
    </article>
</main>
</body>
</html>
