<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Your Parent&#39;s Internet - How to Mitigate Misinformation | Rahul Vishwakarma Blog</title>
    <link rel="stylesheet" type="text/css" href="../styles.css" media="screen" />
</head>
<body>
<header>
    <nav>
        <a href="/index.html" aria-label="Go back to the homepage">‚Üê Back</a>
        <a href="https://blog.boot.dev/misc/online-misinformation-and-censorship/" target="_blank" rel="noopener noreferrer">
            View Original
        </a>
    </nav>
</header>

<main>
    <article>
        <h1>Your Parent&#39;s Internet - How to Mitigate Misinformation</h1>
        <section>
            
            <div id="readability-page-1" class="page"><div>
           <p>The age of information is not what we all hoped it would be. We successfully digitized the majority of human knowledge, and we even made it freely accessible to most. Now the problem is different, we have <em>too much</em> information. Answers to most questions can be found in thousands of distinct places online, and the new problem is ‚Äúwhos information can we trust?‚Äù</p>
<h2 id="what-platforms-think-they-should-do-about-fake-news">
  <span> What platforms think they should do about fake news</span> <a href="#what-platforms-think-they-should-do-about-fake-news">üîó</a></h2>
<p>Twitter and Facebook have recently been under scrutiny for their censorship of coronavirus-related misinformation. For example, a <a href="https://www.bbc.com/news/53559938">video claiming Hydroxychloroquine is a Corona cure</a> recently went viral on Facebook, and the video keeps getting taken down. The video contains some wild assertions, made by Stella Immanuel, who also happens to believe that gynecological problems are the result of <a href="https://www.youtube.com/watch?v=9yCXCP3evAg">spiritual relationships</a>.</p>
<p>By removing content they believe to be dubious, Twitter and Facebook have made themselves arbiters of truth. Anecdotally, all the posts I‚Äôve seen them remove HAVE contained misinformation, but the fact remains‚Ä¶ these platforms have become self-appointed authorities on the veracity of our information.</p>
<p><strong>This is a problem.</strong></p>
<h2 id="so-we-cant-censor">
  <span> So we can‚Äôt censor?</span> <a href="#so-we-cant-censor">üîó</a></h2>
<p>We certainly can, and we certainly should in some cases. Let‚Äôs get some obvious ones out of the way:</p>
<ul>
<li>Child Pornography</li>
<li>Death Threats</li>
<li>Doxing</li>
</ul>
<p>There may be some other clear examples where censoring is unquestionably the right choice, though I doubt there are many. Let‚Äôs look at some more controversial examples:</p>
<ul>
<li>Hate Speech</li>
<li>Misinformation</li>
</ul>
<p>I would posit that here the answer is contingent on <em>who</em> is doing the censoring. While hate speech and misinformation are disgusting, I don‚Äôt want a government deciding <em>what</em> is hate speech, or deciding <em>what</em> is truth.</p>
<p><img src="https://blog.boot.dev/img/800/war-is-peace-300x225.jpg" alt="war is peace 1984 orwell"/></p>
<p>George Orwell, 1984</p>
<p>That said, I certainly want an online system where hate speech and misinformation are effectively filtered out of the conversation. Ideally, every online participant would be a virtuous, educated, and concerned conversationalist. If that were the case, undesirable posts would effectively be ignored due to not receiving the likes, shares, upvotes, and comments they need to spread.</p>
<p>In reality, we can‚Äôt take such a pacifistic approach. We need to <a href="https://www.lesswrong.com/posts/tscc3e5eujrsEeFN4/well-kept-gardens-die-by-pacifism">protect our gardens</a> a bit more.</p>

<p>The steps platforms take are <em>more important</em> than what users do on a personal level to combat misinformation. Platforms can directly affect user behavior patterns that result in more accurate information being circulated through combinations of UI, UX, tooling, and even moderation. As we‚Äôll see in the next section, all individual users can do is try to account for their own biases and look to primary sources.</p>
<p>All online platforms are responsible for the tools they provide for moderation, if not for the moderation itself.</p>
<p>Social platforms <strong>should</strong>:</p>
<ul>
<li>Remove dangerous content such as doxing, threats, child trafficking, etc</li>
<li>Provide tools that allow users to ‚Äúunlike‚Äù or ‚Äúunfollow‚Äù content, even if the unlikes aren‚Äôt visible</li>
<li>Attempt to determine whether a user is saying ‚ÄúI don‚Äôt agree‚Äù vs ‚Äúthis is factually incorrect‚Äù</li>
<li>Posts reported as factually incorrect should have their reach punished, and potentially even be marked as dubious</li>
<li>Users should <strong>probably see more</strong> of the content they don‚Äôt agree with. Platforms should attempt to break down echo chambers of thought.</li>
</ul>
<p>Social platforms <strong>should not</strong> be eager to:</p>
<ul>
<li><em>Remove</em> misleading content</li>
</ul>
<p>By removing misleading content, platforms run the risk of fueling an <a href="https://rationalwiki.org/wiki/Argumentum_ad_martyrdom">argumentum ad martyrdom</a> mentality. Removing information can have an adverse effect, causing people to suspect there is a nefarious reason for removing it.</p>
<blockquote>
<p>But the fact that some geniuses were laughed at does not imply that all who are laughed at are geniuses. They laughed at Columbus, they laughed at Fulton, they laughed at the Wright brothers. But they also laughed at Bozo the Clown.</p>
<p>- Carl Sagan, Probably</p>
</blockquote>
<h2 id="what-should-users-do-about-misinformation">
  <span> What should users do about misinformation?</span> <a href="#what-should-users-do-about-misinformation">üîó</a></h2>
<p>As I mentioned before, this is just a point of educating the userbase. The behaviors below are just good rules of thumb for anyone as a consumer of information. That said, platforms should do more to overtly educate their users about these kinds of critical thinking skills, and even encourage users to put them into practice via reminders or tactful in-app messaging.</p>
<p>Users <strong>should</strong>:</p>
<ul>
<li>Read entire articles before liking, sharing, or commenting</li>
<li>Deploy extra skepticism in regards to information with a clear political or monetary agenda</li>
<li>Attempt to be self-aware about their preconceived notions and confirmation biases</li>
<li>Look for the primary source of information</li>
<li>Ensure information is up-to-date by checking publishing timestamps</li>
</ul>
<p>Users <strong>should not:</strong></p>
<ul>
<li>Reward clickbait titles with engagement</li>
<li>Exclusively follow, subscribe, or search for content that aligns with their current beliefs</li>
<li>Assume their position is valid because people are trying to remove their content</li>
<li>Trust articles and posts coming from sites that appear <a href="https://www.sitelock.com/blog/is-this-website-safe/">unsafe</a></li>
</ul>
  

<div>
  <h3>Find a problem with this article?</h3>
  <p><a target="_blank" href="https://github.com/bootdotdev/blog/issues">Report an issue on GitHub</a>
</p></div>
 
        </div></div>
        </section>
    </article>
</main>
</body>
</html>
