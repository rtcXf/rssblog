<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Data alignment and caches | Rahul Vishwakarma Blog</title>
    <link rel="stylesheet" type="text/css" href="../styles.css" media="screen" />
</head>
<body>
<header>
    <nav>
        <a href="/index.html" aria-label="Go back to the homepage">← Back</a>
        <a href="https://danluu.com/3c-conflict/" target="_blank" rel="noopener noreferrer">
            View Original
        </a>
    </nav>
</header>

<main>
    <article>
        <h1>Data alignment and caches</h1>
        <section>
            
            <div id="readability-page-1" class="page"><div> <p>Here&#39;s the graph of <a href="https://github.com/danluu/setjmp-longjmp-ucontext-snippets/tree/master/benchmark/cache-conflict">a toy benchmark</a> of page-aligned vs. mis-aligned accesses; it shows a ratio of performance between the two at different working set sizes. If this benchmark seems contrived, it actually comes from a real world example of the <a href="https://groups.google.com/forum/#!msg/comp.arch/_uecSnSEQc4/mvfRnOvIyzUJ">disastrous performance implications of using nice power of 2 alignment, or page alignment in an actual system</a>.</p> <p><img src="https://danluu.com/images/3c-conflict/sandy.png" alt="Graph of Sandy Bridge Performance" width="504" height="216"/> <img src="https://danluu.com/images/3c-conflict/westmere.png" alt="Graph of Westmere Performance" width="504" height="504"/></p> <p>Except for very small working sets (1-8), the unaligned version is noticeably faster than the page-aligned version, and there&#39;s a large region up to a working set size of 512 where the ratio in performance is somewhat stable, but more so on our Sandy Bridge chip than our Westmere chip.</p> <p>To understand what&#39;s going on here, we have to look at how caches organize data. By way of analogy, consider a 1,000 car parking garage that has 10,000 permits. With a direct mapped scheme (which you could call 1-way associative), each of the ten permits that has the same 3 least significant digits would be assigned the same spot, i.e., permits 0618, 1618, 2618, and so on, are only allowed to park in spot 618. If you show up at your spot and someone else is in it, you kick them out and they have to drive back home. The next time they get called in to work, they have to drive all the way back to the parking garage.</p> <p>Instead, if each car&#39;s permit allows it to park in a set that has ten possible spaces, we&#39;ll call that a 10-way set associative scheme, which gives us 100 sets of ten spots. Each set is now defined by the last 2 significant digits instead of the last 3. For example, with permit 2618, you can park in any spot from the set {018, 118, 218, …, 918}. If all of them are full, you kick out one unlucky occupant and take their spot, as before.</p> <p>Let&#39;s move out of analogy land and back to our benchmark. The main differences are that there isn&#39;t just one garage-cache, but a hierarchy of them, from the L1, which is the smallest (and hence, fastest) to the L2 and L3. Each seat in a car corresponds to an address. On x86, each addresses points to a particular byte. In the Sandy Bridge chip we&#39;re running on, we&#39;ve got a 32kB L1 cache with 64-byte line size and, 64 sets, with 8-way set associativity. In our analogy, a line size of 64 would correspond to a car with 64 seats. We always transfer things in 64-byte chunks and the bottom log₂(64) = 6 bits of an address refer to a particular byte offset in a cache line. The next log₂(64) = 6 bits determine which set an address falls into. Each of those sets can contain 8 different things, so we have 64 sets * 8 lines/set * 64 bytes/line = 32kB. If we use the cache optimally, we can store 32,768 items. But, since we&#39;re accessing things that are page (4k) aligned, we effectively lose the bottom log₂(4k) = 12 bits, which means that every access falls into the same set, and we can only loop through 8 things before our working set is too large to fit in the L1! But if we&#39;d misaligned our data to different cache lines, we&#39;d be able to use 8 * 64 = 512 locations effectively.</p> <p>Similarly, our chip has a 512 set L2 cache, of which 8 sets are useful for our page aligned accesses, and a 12288 set L3 cache, of which 192 sets are useful for page aligned accesses, giving us 8 sets * 8 lines / set = 64 and 192 sets * 8 lines / set = 1536 useful cache lines, respectively. For data that&#39;s misaligned by a cache line, we have an extra 6 bits of useful address, which means that our L2 cache now has 32,768 useful locations.</p> <p>In the Sandy Bridge graph above, there&#39;s a region of stable relative performance between 64 and 512, as the page-aligned version version is running out of the L3 cache and the unaligned version is running out of the L1. When we pass a working set of 512, the relative ratio gets better for the aligned version because it&#39;s now an L2 access vs. an L3 access. Our graph for Westmere looks a bit different because its L3 is only 3072 sets, which means that the aligned version can only stay in the L3 up to a working set size of 384. After that, we can see the terrible performance we get from spilling into main memory, which explains why the two graphs differ in shape above 384.</p> <p>For a visualization of this, you can think of a 32 bit pointer looking like this to our L1 and L2 caches:</p> <p><code>TTTT TTTT TTTT TTTT TTTT SSSS SSXX XXXX</code></p> <p><code>TTTT TTTT TTTT TTTT TSSS SSSS SSXX XXXX</code></p> <p>The bottom 6 bits are ignored, the next bits determine which set we fall into, and the top bits are a tag that let us know what&#39;s actually in that set. Note that page aligning things, i.e., setting the address to</p> <p><code>???? ???? ???? ???? ???? 0000 0000 0000</code></p> <p>was just done for convenience in our benchmark. Not only will aligning to any large power of 2 cause a problem, generating addresses with a power of 2 offset from each other will cause the same problem.</p> <p>Nowadays, the importance of caches is well understood enough that, when I&#39;m asked to look at a cache related performance bug, it&#39;s usually due to the kind of thing we just talked about: conflict misses that prevent us from using our full cache effectively. This isn&#39;t the only way for that to happen -- bank conflicts and and false dependencies are also common problems, but I&#39;ll leave those for another blog post.</p> <h4 id="resources">Resources</h4> <p>For more on caches on memory, see <a href="http://www.akkadia.org/drepper/cpumemory.pdf">What Every Programmer Should Know About Memory</a>. For something with more breadth, see <a href="https://danluu.com/new-cpu-features/">this blog post for something &#34;short&#34;</a>, or <a href="http://www.amazon.com/gp/product/1478607831/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1478607831&amp;linkCode=as2&amp;tag=abroaview-20&amp;linkId=HXBNFFJ3CIXMWUZP">Modern Processor Design</a> for something book length. For even more breadth (those two links above focus on CPUs and memory), see <a href="http://www.amazon.com/gp/product/012383872X/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=012383872X&amp;linkCode=as2&amp;tag=abroaview-20&amp;linkId=ZHRJAO77SUQ6V7GL">Computer Architecture: A Quantitative Approach</a>, which talks about the whole system up to the datacenter level.</p>  </div></div>
        </section>
    </article>
</main>
</body>
</html>
