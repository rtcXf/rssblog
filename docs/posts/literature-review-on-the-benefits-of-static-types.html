<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Literature review on the benefits of static types | Rahul Vishwakarma Blog</title>
    <link rel="stylesheet" type="text/css" href="../styles.css" media="screen" />
</head>
<body>
<header>
    <nav>
        <a href="/index.html" aria-label="Go back to the homepage">← Back</a>
        <a href="https://danluu.com/empirical-pl/" target="_blank" rel="noopener noreferrer">
            View Original
        </a>
    </nav>
</header>

<main>
    <article>
        <h1>Literature review on the benefits of static types</h1>
        <section>
            
            <div id="readability-page-1" class="page"><div>
<p>There are some pretty strong statements about types floating around out there. The claims range from the oft-repeated phrase that when you get the types to line up, everything just works, to “not relying on type safety is unethical (if you have an SLA)”, &#34;<a href="https://news.ycombinator.com/item?id=4369359">It boils down to cost vs benefit, actual studies, and mathematical axioms, not aesthetics or feelings</a>&#34;, and <a href="https://twitter.com/posco/status/566130704157667328">I think programmers who doubt that type systems help are basically the tech equivalent of an anti-vaxxer</a>. The first and last of these statements are from &#34;types&#34; thought leaders who are widely quoted. There are probably plenty of strong claims about dynamic languages that I&#39;d be skeptical of if I heard them, but I&#39;m not in the right communities to hear the stronger claims about dynamically typed languages. Either way, it&#39;s rare to see people cite actual evidence.</p>

<p>Let&#39;s take a look at the empirical evidence that backs up these claims.</p>



<p><a href="#wat_summary">Click here</a> if you just want to see the summary without having to wade through all the studies. The summary of the summary is that most studies find very small effects, if any. However, the studies probably don&#39;t cover contexts you&#39;re actually interested in. If you want the gory details, here&#39;s each study, with its abstract, and a short blurb about the study.</p>

<h3 id="a-large-scale-study-of-programming-languages-and-code-quality-in-github-ray-b-posnett-d-filkov-v-devanbu-p-http-dl-acm-org-citation-cfm-id-2635922"><a href="http://dl.acm.org/citation.cfm?id=2635922">A Large Scale Study of Programming Languages and Code Quality in Github; Ray, B; Posnett, D; Filkov, V; Devanbu, P</a></h3>

<p><strong>Abstract</strong></p>

<blockquote>
<p>What is the effect of programming languages on software quality? This question has been a topic of much debate for a very long time. In this study, we gather a very large data set from GitHub (729 projects, 80 Million SLOC, 29,000 authors, 1.5 million commits, in 17 languages) in an attempt to shed some empirical light on this question. This reasonably large sample size allows us to use a mixed-methods approach, combining multiple regression modeling with visualization and text analytics, to study the effect of language features such as static v.s. dynamic typing, strong v.s. weak typing on software quality. By triangulating findings from different methods, and controlling for confounding effects such as team size, project size, and project history, we report that language design does have a significant, but modest effect on software quality. Most notably, it does appear that strong typing is modestly better than weak typing, and among functional languages, static typing is also somewhat better than dynamic typing. We also find that functional languages are somewhat better than procedural languages. It is worth noting that these modest effects arising from language design are overwhelmingly dominated by the process factors such as project size, team size, and commit size. However, we hasten to caution the reader that even these modest effects might quite possibly be due to other, intangible process factors, e.g., the preference of certain personality types for functional, static and strongly typed languages.</p>
</blockquote>

<p><strong>Summary</strong></p>

<p>The authors looked at the 50 most starred repos on github for each of the 20 most popular languages plus TypeScript (minus CSS, shell, and vim). For each of these projects, they looked at the languages used. The text in the body of the study doesn&#39;t support the strong claims made in the abstract. Additionally, the study appears to use a fundamentally flawed methodology that&#39;s not capable of revealing much information. Even if the methodology were sound, the study uses bogus data and has what Pinker calls the <a href="http://languagelog.ldc.upenn.edu/nll/?p=1897">igon value problem</a>.</p>

<p>As Gary Bernhardt points out, the authors of the study <a href="https://twitter.com/garybernhardt/status/862907601557651457">seem to confuse memory safety and implicit coercion</a> and <a href="https://twitter.com/garybernhardt/status/862907391536279554">make other strange statements</a>, such as</p>

<blockquote>
<p>Advocates of dynamic typing may argue that rather than spend a lot of time correcting annoying static type errors arising from sound, conservative static type checking algorithms in compilers, it’s better to rely on strong dynamic typing to catch errors as and when they arise.</p>
</blockquote>

<p>The study uses the following language classification scheme</p>

<p><img src="https://danluu.com/images/empirical-pl/lang_classes.png" alt="Table of classifications"/></p>

<p>These classifications seem arbitrary and many people would disagree with some of these classifications. Since the results are based on aggregating results with respect to these categories, and the authors have chosen arbitrary classifications, this already makes the aggragated results suspect since they have a number of degrees of freedom here and they&#39;ve made some odd choicses.</p>

<p>In order to get the language level results, the authors looked at commit/PR logs to determine how many bugs there were for each language used. As far as I can tell, open issues with no associated fix don&#39;t count towards the bug count. Only commits that are detected by their keyword search technique were counted. With this methodology, the number of bugs found will depend at least as strongly on the bug reporting culture as it does on the actual number of bugs found.</p>

<p>After determining the number of bugs, the authors ran a regression, controlling for project age, number of developers, number of commits, and lines of code.</p>

<p><img src="https://danluu.com/images/empirical-pl/lang_defects.png" alt="Defect rate correlations"/></p>

<p>There are enough odd correlations here that, even if the methodology wasn&#39;t known to be flawed, I&#39;d be skeptical that authors have captured a causal relationship. If you don&#39;t find it odd that Perl and Ruby are as reliable as each other and significantly more reliable than Erlang and Java (which are also equally reliable), which are significantly more reliable than Python, PHP, and C (which are similarly reliable), and that TypeScript is the safest language surveyed, then maybe this passes the sniff test for you, but even without reading further, this looks suspicious.</p>

<p>For example, Erlang and Go are rated as having a lot of concurrency bugs, whereas Perl and CoffeeScript are rated as having few concurrency bugs. Is it more plausible that Perl and CoffeeScript are better at concurrency than Erlang and Go or that people tend to use Erlang and Go more when they need concurrency? The authors note that Go might have a lot of concurrency bugs because there&#39;s a good tool to detect concurrency bugs in Go, but they don&#39;t explore reasons for most of the odd intermediate results.</p>

<p>As for TypeScript, Eirenarch has pointed out that the three projects they list as example TypeScript projects, which they call the &#34;top three&#34; TypeScript projects are bitcoin, litecoin, and qBittorrent). <a href="http://bitcoin.stackexchange.com/questions/22311/why-does-github-say-that-the-bitcoin-project-is-74-typescript">These are C++ projects</a>. So the intermediate result appears to not be that TypeScript is reliable, but that projects mis-identified as TypeScript are reliable. Those projects are reliable because Qt translation files are identified as TypeScript and it turns out that, per line of code, giant dumps of config files from another project don&#39;t cause a lot of bugs. It&#39;s like saying that a project has few bugs per line of code because it has a giant README. This is the most blatant classification error, but it&#39;s far from the only one.</p>

<p>For example, of what they call the &#34;top three&#34; perl projects, one is showdown, a javascript project, and one is rails-dev-box, a shell script and a vagrant file used to launch a Rails dev environment. Without knowing anything about the latter project, one might expect it&#39;s not a perl project from its name, rails-dev-box, which correctly indicates that it&#39;s a rails related project.</p>

<p>Since this study uses Github&#39;s notoriously inaccurate code classification system to classify repos, it is, at best, a series of correlations with factors that are themselves only loosely correlated with actual language usage.</p>

<p>There&#39;s more analysis, but much of it is based on aggregating the table above into categories based on language type. Since I&#39;m skeptical of these results, I&#39;m at least as skeptical of any results based on aggregating these results. This section barely even scratches the surface of this study. Even with just a light skim, we see multiple serious flaws, any one of which would invalidate the results, plus numerous <a href="http://languagelog.ldc.upenn.edu/nll/?p=1897">igon value problems</a>. It appears that the authors didn&#39;t even look at the tables they put in the paper, since if they did, it would jump out that (just for example), they classified a project called &#34;rails-dev-box&#34; as one of the three biggest perl projects (it&#39;s a 70-line shell script used to spin up ruby/rails dev environments).</p>

<h3 id="do-static-type-systems-improve-the-maintainability-of-software-systems-an-empirical-study-kleinschmager-s-hanenberg-s-robbes-r-tanter-e-stefik-a-http-pleiad-dcc-uchile-cl-papers-2012-kleinschmageral-icpc2012-pdf"><a href="http://pleiad.dcc.uchile.cl/papers/2012/kleinschmagerAl-icpc2012.pdf">Do Static Type Systems Improve the Maintainability of Software Systems? An Empirical Study Kleinschmager, S.; Hanenberg, S.; Robbes, R.; Tanter, E.; Stefik, A.</a></h3>

<p><strong>Abstract</strong></p>

<blockquote>
<p>Static type systems play an essential role in contemporary programming languages. Despite their importance, whether static type systems influence human software development capabilities remains an open question. One frequently mentioned argument for static type systems is that they improve the maintainability of software systems - an often used claim for which there is no empirical evidence. This paper describes an experiment which tests whether static type systems improve the maintainability of software systems. The results show rigorous empirical evidence that static type are indeed beneficial to these activities, except for fixing semantic errors.</p>
</blockquote>

<p><strong>Summary</strong></p>

<p>While the abstract talks about general classes of languages, the study uses Java and Groovy.</p>

<p>Subjects were given classes in which they had to either fix errors in existing code or fill out  stub methods. Static classes for Java, dynamic classes for Groovy. In cases of type errors (and their respective no method errors), developers solved the problem faster in Java. For semantic errors, there was no difference.</p>

<p>The study used a <a href="https://en.wikipedia.org/wiki/Repeated_measures_design">within-subject design</a>, with randomized task order over 33 subjects.</p>

<p>A notable limitation is that the study avoided using “complicated control structures”, such as loops and recursion, because those increase variance in time-to-solve. As a result, all of the bugs are trivial bugs. This can be seen in the median time to solve the tasks, which are in the hundreds of seconds. Tasks can include multiple bugs, so the time per bug is quite low.</p>

<p><img src="https://danluu.com/images/empirical-pl/groovy_v_java.png" alt="Groovy is both better and worse than Java"/></p>

<p>This paper mentions that its results contradict some prior results, and one of the possible causes they give is that their tasks are more complex than the tasks from those other papers. The fact that the tasks in this paper don&#39;t involve using loops and recursion because they&#39;re too complicated, should give you an idea of the complexity of the tasks involved in most of these papers.</p>

<p>Other limitations in this experiment were that the variables were artificially named such that there was no type information encoded in any of the names, that there were no comments, and that there was zero documentation on the APIs provided. That&#39;s an unusually hostile environment to find bugs in, and it&#39;s not clear how the results generalize if any form of documentation is provided.</p>

<p>Additionally, even though the authors specifically picked trivial tasks in order to minimize the variance between programmers, the variance between programmers was still much greater than the variance between languages in all but two tasks. Those two tasks were both cases of a simple type error causing a run-time exception that wasn&#39;t near the type error.</p>

<h3 id="a-controlled-experiment-to-assess-the-benefits-of-procedure-argument-type-checking-prechelt-l-tichy-w-f-http-ieeexplore-ieee-org-xpl-articledetails-jsp-arnumber-677186"><a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=677186">A controlled experiment to assess the benefits of procedure argument type checking, Prechelt, L.; Tichy, W.F.</a></h3>

<p><strong>Abstract</strong></p>

<blockquote>
<p>Type checking is considered an important mechanism for detecting programming errors, especially interface errors. This report describes an experiment to assess the defect-detection capabilities of static, intermodule type checking.</p>

<p>The experiment uses ANSI C and Kernighan &amp; Ritchie (K&amp;R) C. The relevant difference is that the ANSI C compiler checks module interfaces (i.e., the parameter lists calls to external functions), whereas K&amp;R C does not. The experiment employs a counterbalanced design in which each of the 40 subjects, most of them CS PhD students, writes two nontrivial programs that interface with a complex library (Motif). Each subject writes one program in ANSI C and one in K&amp;R C. The input to each compiler run is saved and manually analyzed for defects.</p>

<p>Results indicate that delivered ANSI C programs contain significantly fewer interface defects than delivered K&amp;R C programs. Furthermore, after subjects have gained some familiarity with the interface they are using, ANSI C programmers remove defects faster and are more productive (measured in both delivery time and functionality implemented)</p>
</blockquote>

<p><strong>Summary</strong></p>

<p>The “nontrivial” tasks are the inversion of a 2x2 matrix (with GUI) and a file “browser” menu that has two options, select file and display file. Docs for motif were provided, but example code was deliberately left out.</p>

<p>There are 34 subjects. Each subjects solves one problem with the K&amp;R C compiler (which doesn&#39;t typecheck arguments) and one with the ANSI C compiler (which does).</p>

<p>The authors note that the distribution of results is non-normal, with highly skewed outliers, but they present their results as box plots, which makes it impossible to see the distribution. They do some statistical significance tests on various measures, and find no difference in time to completion on the first task, a significant difference on the second task, but no difference when the tasks are pooled.</p>

<p><img src="https://danluu.com/images/empirical-pl/mysterious_boxplot.png" alt="ANSI C is better, except when it&#39;s worse"/></p>

<p>In terms of how the bugs are introduced during the programming process, they do a significance test against the median of one measure of defects (which finds a significant difference in the first task but not the second), and a significance test against the 75%-quantile of another measure (which finds a significant difference in the second task but not the first).</p>

<p>In terms of how many and what sort of bugs are in the final program, they define a variety of measures and find that some differences on the measures are statistically significant and some aren&#39;t. In the table below, bolded values indicate statistically significant differences.</p>

<p><img src="https://danluu.com/images/empirical-pl/ansi_kr_table.png" alt="Breakdown by various metrics"/></p>

<p>Note that here, first task refers to whichever task the subject happened to perform first, which is randomized, which makes the results seem rather arbitrary. Furthermore, the numbers they compare are medians (except where indicated otherwise), which also seems arbitrary.</p>

<p>Despite the strong statement in the abstract, I&#39;m not convinced this study presents strong evidence for anything in particular. They have <a href="https://en.wikipedia.org/wiki/Multiple_comparisons_problem">multiple comparisons</a>, many of which seem arbitrary, and find that some of them are significant. They also find that many of their criteria don&#39;t have significant differences. Furthermore, they don&#39;t mention whether or not they tested any other arbitrary criteria. If they did, the results are much weaker than they look, and they already don&#39;t look strong.</p>

<p>My interpretation of this is that, if there is an effect, the effect is dwarfed by the difference between programmers, and it&#39;s not clear whether there&#39;s any real effect at all.</p>

<h3 id="an-empirical-comparison-of-c-c-java-perl-python-rexx-and-tcl-prechelt-l-http-page-mi-fu-berlin-de-prechelt-biblio-jccpprt-computer2000-pdf"><a href="http://page.mi.fu-berlin.de/prechelt/Biblio/jccpprt_computer2000.pdf">An empirical comparison of C, C++, Java, Perl, Python, Rexx, and Tcl, Prechelt, L.</a></h3>

<p><strong>Abstract</strong></p>

<blockquote>
<p>80 implementations of the same set of requirements are compared for several properties, such as run time, memory consumption, source text length, comment density, program structure, reliability, and the amount of effort required for writing them. The results indicate that, for the given programming problem, which regards string manipulation and search in a dictionary, “scripting languages” (Perl, Python, Rexx, Tcl) are more productive than “conventional languages” (C, C++, Java). In terms of run time and memory consumption, they often turn out better than Java and not much worse than C or C++. In general, the differences between languages tend to be smaller than the typical differences due to different programmers within the same language.</p>
</blockquote>

<p><strong>Summary</strong></p>

<p>The task was to read in a list of phone numbers and return a list of words that those phone numbers could be converted to, using the letters on a phone keypad.</p>

<p>This study was done in two phases. There was a controlled study for the C/C++/Java group, and a self-timed implementation for the Perl/Python/Rexx/Tcl group. The former group consisted of students while the latter group consisted of respondents from a newsgroup. The former group received more criteria they should consider during implementation, and had to implement the program when they received the problem description, whereas some people in the latter group read the problem description days or weeks before implementation.</p>

<p><img src="https://danluu.com/images/empirical-pl/yellow_runtime.png" alt="C and C++ are fast"/>
<img src="https://danluu.com/images/empirical-pl/yellow_devtime.png" alt="C, C++, and Java are slow to write"/></p>

<p>If you take the results at face value, it looks like the class of language used imposes a lower bound on both implementation time and execution time, but that the variance between programmers is much larger than the variance between languages.</p>

<p>However, since the scripting language group had significantly different (and easier) environment than the C-like language group, it&#39;s hard to say how much of the measured difference in implementation time is from flaws in the experimental design and how much is real.</p>

<h3 id="static-type-systems-sometimes-have-a-positive-impact-on-the-usability-of-undocumented-software-mayer-c-hanenberg-s-robbes-r-tanter-e-stefik-a-http-swp-dcc-uchile-cl-tr-2012-tr-dcc-20120418-005-pdf"><a href="http://swp.dcc.uchile.cl/TR/2012/TR_DCC-20120418-005.pdf">Static type systems (sometimes) have a positive impact on the usability of undocumented software; Mayer, C.; Hanenberg, S.; Robbes, R.; Tanter, E.; Stefik, A.</a></h3>

<p><strong>Abstract</strong></p>

<blockquote>
<p>Static and dynamic type systems (as well as more recently gradual type systems) are an important research topic in programming language design. Although the study of such systems plays a major role in research, relatively little is known about the impact of type systems on software development. Perhaps one of the more common arguments for static type systems is that they require developers to annotate their code with type names, which is thus claimed to improve the documentation of software. In contrast, one common argument against static type systems is that they decrease flexibility, which may make them harder to use. While positions such as these, both for and against static type systems, have been documented in the literature, there is little rigorous empirical evidence for or against either position. In this paper, we introduce a controlled experiment where 27 subjects performed programming tasks on an undocumented API with a static type system (which required type annotations) as well as a dynamic type system (which does not). Our results show that for some types of tasks, programmers were afforded faster task completion times using a static type system, while for others, the opposite held. In this work, we document the empirical evidence that led us to this conclusion and conduct an exploratory study to try and theorize why.</p>
</blockquote>

<p><strong>Summary</strong></p>

<p>The experimental setup is very similar to the previous Hanenberg paper, so I&#39;ll just describe the main difference, which is that subjects used either Java, or a restricted subset of Groovy that was equivalent to dynamically typed Java. Subjects were students who had previous experience in Java, but not Groovy, giving some advantage for the Java tasks.</p>

<p><img src="https://danluu.com/images/empirical-pl/groovy_v_java_2.png" alt="Groovy is both better and worse than Java"/></p>

<p>Task 1 was a trivial warm-up task. The authors note that it&#39;s possible that Java is superior on task 1 because the subjects had prior experience in Java. The authors speculate that, in general, Java is superior to untyped Java for more complex tasks, but they make it clear that they&#39;re just speculating and don&#39;t have enough data to conclusively support that conclusion.</p>

<h3 id="how-do-api-documentation-and-static-typing-affect-api-usability-endrikat-s-hanenberg-s-robbes-romain-stefik-a-http-users-dcc-uchile-cl-rrobbes-p-icse2014-docstypes-pdf"><a href="http://users.dcc.uchile.cl/~rrobbes/p/ICSE2014-docstypes.pdf">How Do API Documentation and Static Typing Affect API Usability? Endrikat, S.; Hanenberg, S.; Robbes, Romain; Stefik, A.</a></h3>

<p><strong>Abstract</strong></p>

<blockquote>
<p>When developers use Application Programming Interfaces (APIs), they often rely on documentation to assist their tasks. In previous studies, we reported evidence indicating that static type systems acted as a form of implicit documentation, benefiting developer productivity. Such implicit documentation is easier to maintain, given it is enforced by the compiler, but previous experiments tested users without any explicit documentation. In this paper, we report on a controlled experiment and an exploratory study comparing the impact of using documentation and a static or dynamic type system on a development task. Results of our study both confirm previous findings and show that the benefits of static typing are strengthened with explicit documentation, but that this was not as strongly felt with dynamically typed languages.</p>
</blockquote>

<p>There&#39;s an earlier study in this series with the following abstract:</p>

<blockquote>
<p>In the discussion about the usefulness of static or dynamic type systems there is often the statement that static type systems improve the documentation of software. In the meantime there exists even some empirical evidence for this statement. One of the possible explanations for this positive influence is that the static type system of programming languages such as Java require developers to write down the type names, i.e. lexical representations which potentially help developers. Because of that there is a plausible hypothesis that the main benefit comes from the type names and not from the static type checks that are based on these names. In order to argue for or against static type systems it is desirable to check this plausible hypothesis in an experimental way. This paper describes an experiment with 20 participants that has been performed in order to check whether developers using an unknown API already benefit (in terms of development time) from the pure syntactical representation of type names without static type checking. The result of the study is that developers do benefit from the type names in an API&#39;s source code. But already a single wrong type name has a measurable significant negative impact on the development time in comparison to APIs without type names.</p>
</blockquote>

<p>The languages used were Java and Dart. The university running the tests teaches in Java, so subjects had prior experience in Java. The task was one “where participants use the API in a way that objects need to be configured and passed to the API”, which was chosen because the authors thought that both types and documentation should have some effect. “The challenge for developers is to locate all the API elements necessary to properly configure [an] object”. The documentation was free-form text plus examples.</p>

<p><img src="https://danluu.com/images/empirical-pl/java_v_dart.png" alt="Documentation is only helpful with types and vice versa?"/></p>

<p>Taken at face value, it looks like types+documentation is a lot better than having one or the other, or neither. But since the subjects were students at a school that used Java, it&#39;s not clear how much of the effect is from familiarity with the language and how much is from the language. Moreover, the task was a single task that was chosen specifically because it was the kind of task where both types and documentation were expected to matter.</p>

<h3 id="an-experiment-about-static-and-dynamic-type-systems-hanenberg-s-http-courses-cs-washington-edu-courses-cse590n-10au-hanenberg-oopsla2010-pdf"><a href="http://courses.cs.washington.edu/courses/cse590n/10au/hanenberg-oopsla2010.pdf">An Experiment About Static and Dynamic Type Systems; Hanenberg, S.</a></h3>

<p><strong>Abstract</strong></p>

<blockquote>
<p>Although static type systems are an essential part in teaching and research in software engineering and computer science, there is hardly any knowledge about what the impact of static type systems on the development time or the resulting quality for a piece of software is. On the one hand there are authors that state that static type systems decrease an application&#39;s complexity and hence its development time (which means that the quality must be improved since developers have more time left in their projects). On the other hand there are authors that argue that static type systems increase development time (and hence decrease the code quality) since they restrict developers to express themselves in a desired way. This paper presents an empirical study with 49 subjects that studies the impact of a static type system for the development of a parser over 27 hours working time. In the experiments the existence of the static type system has neither a positive nor a negative impact on an application&#39;s development time (under the conditions of the experiment).</p>
</blockquote>

<p><strong>Summary</strong></p>

<p>This is another Hanenberg study with a basically sound experimental design, so I won&#39;t go into details about the design. Some unique parts are that, in order to control for familiarity and other things that are difficult to control for with existing languages, the author created two custom languages for this study.</p>

<p>The author says that the language has similarities to Smalltalk, Ruby, and Java, and that the language is a class-based OO language with single implementation inheritance and late binding.</p>

<p>The students had 16 hours of training in the new language before starting. The author argues that this was sufficient because “the language, its API as well as its IDE was kept very simple”. An additional 2 hours was spent to explain the type system for the static types group.</p>

<p>There were two tasks, a “small” one (implementing a scanner) and a “large” one (implementing a parser). The author found a statistically significant difference in time to complete the small task (the dynamic language was faster) and no difference in the time to complete the large task.</p>

<p>There are a number of reasons this result may not be generalizable. The author is aware of them and there&#39;s a long section on ways this study doesn&#39;t generalize as well as a good discussion on threats to validity.</p>

<h3 id="work-in-progress-an-empirical-study-of-static-typing-in-ruby-daly-m-sazawal-v-foster-j-http-www-cs-umd-edu-jfoster-papers-plateau09-ruby-pdf"><a href="http://www.cs.umd.edu/~jfoster/papers/plateau09-ruby.pdf">Work In Progress: an Empirical Study of Static Typing in Ruby; Daly, M; Sazawal, V; Foster, J.</a></h3>

<p><strong>Abstract</strong></p>

<blockquote>
<p>In this paper, we present an empirical pilot study of four skilled programmers as they develop programs in Ruby, a popular, dynamically typed, object-oriented scripting language. Our study compares programmer behavior under the standard Ruby interpreter versus using Diamondback Ruby (DRuby), which adds static type inference to Ruby. The aim of our study is to understand whether DRuby&#39;s static typing is beneficial to programmers. We found that DRuby&#39;s warnings rarely provided information about potential errors not already evident from Ruby&#39;s own error messages or from presumed prior knowledge. We hypothesize that programmers have ways of reasoning about types that compensate for the lack of static type information, possibly limiting DRuby&#39;s usefulness when used on small programs.</p>
</blockquote>

<p><strong>Summary</strong></p>

<p>Subjects came from a local Ruby user&#39;s group. Subjects implemented a simplified Sudoku solver and a maze solver. DRuby was randomly selected for one of the two problems for each subject. There were four subjects, but the authors changed the protocol after the first subject. Only three subjects had the same setup.</p>

<p>The authors find no benefit to having types. This is one of the studies that the first Hanenberg study mentions as a work their findings contradict. That first paper claimed that it was because their tasks were more complex, but it seems to me that this paper has a more complex task. One possible reason they found contradictory results is that the effect size is small. Another is that the specific type systems used matter, and that a DRuby v. Ruby study doesn&#39;t generalize to Java v. Groovy. Another is that the previous study attempted to remove anything hinting at type information from the dynamic implementation, including names that indicate types and API documentation. The participants of this study mention that they get a lot of type information from API docs, and the authors note that the participants encode type information in their method names.</p>

<p>This study was presented in a case study format, with selected comments from the participants and an analysis of their comments. The authors note that participants regularly think about types, and check types, even when programming in a dynamic language.</p>

<h3 id="haskell-vs-ada-vs-c-vs-awk-vs-an-experiment-in-software-prototyping-productivity-hudak-p-jones-m-http-haskell-cs-yale-edu-post-type-publication-p-366"><a href="http://haskell.cs.yale.edu/?post_type=publication&amp;p=366">Haskell vs. Ada vs. C++ vs. Awk vs. ... An Experiment in Software Prototyping Productivity; Hudak, P; Jones, M.</a></h3>

<p><strong>Abstract</strong></p>

<blockquote>
<p>We describe the results of an experiment in which several conventional programming languages, together with the functional language Haskell, were used to prototype a Naval Surface Warfare Center (NSWC) requirement for a Geometric Region Server.  The resulting programs and development metrics were reviewed by a committee chosen by the Navy.  The results indicate that the Haskell prototype took significantly less time to develop and was considerably more concise and easier to understand than the corresponding prototypes written in several different imperative languages, including Ada and C++.</p>
</blockquote>

<p><strong>Summary</strong></p>

<p>Subjects were given an informal text description for the requirements of a geo server. The requirements were behavior oriented and didn&#39;t mention performance. The subjects were “expert” programmers in the languages they used. They were asked to implement a prototype and track metrics such as dev time, lines of code, and docs. Metrics were all self reported, and no guidelines were given as to how they should be measured, so metrics varied between subjects. Also, some, but not all, subjects attended a meeting where additional information was given on the assignment.</p>

<p>Due to the time-frame and funding requirements, the requirements for the server were extremely simple; the median implementation was a couple hundred lines of code. Furthermore, the panel that reviewed the solutions didn&#39;t have time to evaluate or run the code; they based their findings on the written reports and oral presentations of the subjects.</p>

<p><img src="https://danluu.com/images/empirical-pl/hudak_table.png" alt="Table of LOC, dev time, and lines of code"/></p>

<p>This study hints at a very interesting result, but considering all of its limitations, the fact that each language (except Haskell) was only tested once, and that other studies show much larger intra-group variance than inter-group variance, it&#39;s hard to conclude much from this study alone.</p>

<h3 id="unit-testing-isn-t-enough-you-need-static-typing-too-farrer-e-http-evanfarrer-blogspot-com-2012-06-unit-testing-isnt-enough-you-need-html"><a href="http://evanfarrer.blogspot.com/2012/06/unit-testing-isnt-enough-you-need.html">Unit testing isn&#39;t enough. You need static typing too; Farrer, E</a></h3>

<p><strong>Abstract</strong></p>

<blockquote>
<p>Unit testing and static type checking are tools for ensuring defect free software. Unit testing is the practice of writing code to test individual units of a piece of software. By validating each unit of software, defects can be discovered during development. Static type checking is performed by a type checker that automatically validates the correct typing of expressions and statements at compile time. By validating correct typing, many defects can be discovered during development. Static typing also limits the expressiveness of a programming language in that it will reject some programs which are ill-typed, but which are free of defects.</p>

<p>Many proponents of unit testing claim that static type checking is an insufficient mechanism for ensuring defect free software; and therefore, unit testing is still required if static type checking is utilized. They also assert that once unit testing is utilized, static type checking is no longer needed for defect detection, and so it should be eliminated.</p>

<p>The goal of this research is to explore whether unit testing does in fact obviate static type checking in real world examples of unit tested software.</p>
</blockquote>

<p><strong>Summary</strong></p>

<p>The author took four Python programs and translated them to Haskell. Haskell&#39;s type system found some bugs. Unlike academic software engineering research, this study involves something larger than a toy program and looks at a type system that&#39;s more expressive than Java&#39;s type system. The programs were the NMEA Toolkit (9 bugs), MIDITUL (2 bugs), GrapeFruit (0 bugs), and PyFontInfo (6 bugs).</p>

<p>As far as I can tell, there isn&#39;t an analysis of the severity of the bugs. The programs were 2324, 2253, 2390, and 609 lines long, respectively, so the bugs found / LOC were 17 / 7576 = 1 / 446. For reference, in Code Complete, Steve McConnell estimates that 15-50 bugs per 1kLOC is normal. If you believe that estimate applies to this codebase, you&#39;d expect that this technique caught between 4% and 15% of the bugs in this code. There&#39;s no particular reason to believe the estimate should apply, but we can keep this number in mind as a reference in order to compare to a similarly generated number from another study that we&#39;ll get to later.</p>

<p>The author does some analysis on how hard it would have been to find the bugs through testing, but only considers line coverage directed unit testing; the author comments that bugs might have have been caught by unit testing if they could be missed with 100% line coverage. This seems artificially weak — it&#39;s generally well accepted that line coverage is a very weak notion of coverage and that testing merely to get high line coverage isn&#39;t sufficient. In fact, it is generally <a href="http://blog.regehr.org/archives/386">considered insufficient to even test merely to get high path coverage</a>, which is a much stronger notion of coverage than line coverage.</p>

<h3 id="gradual-typing-of-erlang-programs-a-wrangler-experience-sagonas-k-luna-d-http-www-it-uu-se-research-group-hipe-dialyzer-publications-wrangler-pdf"><a href="http://www.it.uu.se/research/group/hipe/dialyzer/publications/wrangler.pdf">Gradual Typing of Erlang Programs: A Wrangler Experience; Sagonas, K; Luna, D</a></h3>

<p><strong>Abstract</strong></p>

<blockquote>
<p>Currently most Erlang programs contain no or very little type information. This sometimes makes them unreliable, hard to use, and difficult to understand and maintain. In this paper we describe our experiences from using static analysis tools to gradually add type information to a medium sized Erlang application that we did not write ourselves: the code base of Wrangler. We carefully document the approach we followed, the exact steps we took, and discuss possible difficulties that one is expected to deal with and the effort which is required in the process. We also show the type of software defects that are typically brought forward, the opportunities for code refactoring and improvement, and the expected benefits from embarking in such a project. We have chosen Wrangler for our experiment because the process is better explained on a code base which is small enough so that the interested reader can retrace its steps, yet large enough to make the experiment quite challenging and the experiences worth writing about. However, we have also done something similar on large parts of Erlang/OTP. The result can partly be seen in the source code of Erlang/OTP R12B-3.</p>
</blockquote>

<p><strong>Summary</strong></p>

<p>This is somewhat similar to the study in “Unit testing isn&#39;t enough”, except that the authors of this study created a static analysis tool instead of translating the program into another language. The authors note that they spent about half an hour finding and fixing bugs after running their tool. They also point out some bugs that would be difficult to find by testing. They explicitly state “what&#39;s interesting in our approach is that all these are achieved without imposing any (restrictive) static type system in the language.” The authors have a follow-on paper, “Static Detection of Race Conditions in Erlang”, which extends the approach.</p>

<p>The list of papers that find bugs using static analysis without explicitly adding types is too long to list. This is just one typical example.</p>

<h3 id="0install-replacing-python-leonard-t-http-roscidus-com-blog-blog-2013-06-09-choosing-a-python-replacement-for-0install-pt2-http-roscidus-com-blog-blog-2013-06-20-replacing-python-round-2-pt3-http-roscidus-com-blog-blog-2014-02-13-ocaml-what-you-gain"><a href="http://roscidus.com/blog/blog/2013/06/09/choosing-a-python-replacement-for-0install/">0install: Replacing Python; Leonard, T.</a>, <a href="http://roscidus.com/blog/blog/2013/06/20/replacing-python-round-2/">pt2</a>, <a href="http://roscidus.com/blog/blog/2014/02/13/ocaml-what-you-gain/">pt3</a></h3>

<p><strong>Abstract</strong></p>

<p>No abstract because this is a series of blog posts.</p>

<p><strong>Summary</strong></p>

<p>This compares ATS, C#, Go, Haskell, OCaml, Python and Rust. The author assigns scores to various criteria, but it&#39;s really a qualitative comparison. But it&#39;s interesting reading because it seriously considers the effect of language on a non-trivial codebase (30kLOC).</p>

<p>The author implemented parts of 0install in various languages and then eventually decided on Ocaml and ported the entire thing to Ocaml. There are some great comments about why the author chose Ocaml and what the author gained by using Ocaml over Python.</p>

<h3 id="verilog-vs-vhdl-design-competition-cooley-j-verilog-vs-vhdl"><a href="https://danluu.com/verilog-vs-vhdl/">Verilog vs. VHDL design competition; Cooley, J</a></h3>

<p><strong>Abstract</strong></p>

<p>No abstract because it&#39;s a usenet posting</p>

<p><strong>Summary</strong></p>

<p>Subjects were given 90 minutes to create a small chunk of hardware, a synchronous loadable 9-bit increment-by-3
decrement-by-5 up/down counter that generated even parity, carry and borrow, with the goal of optimizing for cycle time of the synthesized result. For the software folks reading this, this is something you&#39;d expect to be able to do in 90 minutes if nothing goes wrong, or maybe if only a few things go wrong.</p>

<p>Subjects were judged purely by how optimized their result was, as long as it worked. Results that didn&#39;t pass all tests were disqualified. Although the task was quite simple, it was made substantially more complicated by the strict optimization goal. For any software readers out there, this task is approximately as complicated as implementing the same thing in assembly, where your assembler takes 15-30 minutes to assemble something.</p>

<p>Subjects could use Verilog (unityped) or VHDL (typed). 9 people chose Verilog and 5 chose VHDL.</p>

<p>During the expierment, there were a number of issues that made things easier or harder for some subjects. Overall, Verilog users were affected more negatively than VHDL users. The license server for the Verilog simulator crashed. Also, four of the five VHDL subjects were accidentally given six extra minutes. The author had manuals for the wrong logic family available, and one Verilog user spent 10 minutes reading the wrong manual before giving up and using his intuition. One of the Verilog users noted that they passed the wrong version of their code along to be tested and failed because of that. One of the VHDL users hit a bug in the VHDL simulator.</p>

<p>Of the 9 Verilog users, 8 got something synthesized before the 90 minute deadline; of those, 5 had a design that passed all tests. None of the VHDL users were able to synthesize a circuit in time.</p>

<p>Two of the VHDL users complained about issues with types “I can&#39;t believe I got caught on a simple typing error. I used IEEE std_logic_arith, which requires use of unsigned &amp; signed subtypes, instead of std_logic_unsigned.”, and &#34;I ran into a problem with VHDL or VSS (I&#39;m still not sure.)  This case statement doesn&#39;t analyze: ‘subtype two_bits is unsigned(1 downto 0); case two_bits&#39;(up &amp; down)...&#39;  But what worked was: ‘case two_bits&#39;(up, down)...&#39;  Finally I solved this problem by assigning the concatenation first to a[n] auxiliary variable.&#34;</p>

<h3 id="comparing-mathematical-provers-wiedijk-f-http-www-cs-ru-nl-f-wiedijk-comparison-diffs-pdf"><a href="http://www.cs.ru.nl/F.Wiedijk/comparison/diffs.pdf">Comparing mathematical provers; Wiedijk, F</a></h3>

<p><strong>Abstract</strong></p>

<blockquote>
<p>We compare fifteen systems for the formalizations of mathematics with the computer. We present several tables that list various properties of these programs. The three main dimensions on which we compare these systems are: the size of their library, the strength of their logic and their level of automation.</p>
</blockquote>

<p><strong>Summary</strong></p>

<p>The author compares the type systems and foundations of various theorem provers, and comments on their relative levels of proof automation.</p>

<p><img src="https://danluu.com/images/empirical-pl/prover_types.png" alt="Type systems of provers"/>
<img src="https://danluu.com/images/empirical-pl/prover_foundations.png" alt="Foundations of provers"/>
<img src="https://danluu.com/images/empirical-pl/prover_graph.png" alt="Graph of automation level of provers"/></p>

<p>The author looked at one particular problem (proving the irrationality of the square root of two) and examined how different systems handle the problem, including the style of the proof and its length. There&#39;s a table of lengths, but it doesn&#39;t match the <a href="http://www.cs.ru.nl/~freek/comparison/">updated code examples provided here</a>. For instance, that table claims that the ACL2 proof is 206 lines long, but there&#39;s a <a href="http://www.cs.ru.nl/~freek/comparison/files/acl2.lisp">21 line ACL2 proof here</a>.</p>

<p>The author has a number of criteria for determining how much automation prover provides, but he freely admits that it&#39;s highly subjective. The author doesn&#39;t provide the exact rubric used for scoring, but he mentions that a more automated interaction style, user automation, powerful built-in automation, and the Poincare principle (basically whether the system lets you write programs to solve proofs algorithmically) all count towards being more automated, and more powerful logic (e.g., first-order v. higher-order), logical framework dependent types, and de Bruijn criterion (having a small guaranteed kernel) count towards being more mathematical.</p>

<h3 id="do-programming-languages-affect-productivity-a-case-study-using-data-from-open-source-projects-delory-d-knutson-c-chun-s-http-sequoia-cs-byu-edu-lab-files-pubs-delorey2007a-pdf"><a href="http://sequoia.cs.byu.edu/lab/files/pubs/Delorey2007a.pdf">Do Programming Languages Affect Productivity? A Case Study Using Data from Open Source Projects; Delory, D; Knutson, C; Chun, S</a></h3>

<p><strong>Abstract</strong></p>

<blockquote>
<p>Brooks and others long ago suggested that on average computer programmers write the same number of lines of code in a given amount of time regardless of the programming language used. We examine data collected from the CVS repositories of 9,999 open source projects hosted on SourceForge.net to test this assump- tion for 10 of the most popular programming languages in use in the open source community. We find that for 24 of the 45 pairwise comparisons, the programming language is a significant factor in determining the rate at which source code is written, even after accounting for variations between programmers and projects.</p>
</blockquote>

<p><strong>Summary</strong></p>

<p>The authors say “our goal is not to construct a predictive or explanatory model. Rather, we seek only to develop a model that sufficiently accounts for the variation in our data so that we may test the significance of the estimated effect of programming language.” and that&#39;s what they do. They get some correlations, but it&#39;s hard to conclude much of anything from them.</p>

<h3 id="the-unreasonable-effectiveness-of-dynamic-typing-for-practical-programs-smallshire-r-http-vimeo-com-74354480"><a href="http://vimeo.com/74354480">The Unreasonable Effectiveness of Dynamic Typing for Practical Programs; Smallshire, R</a></h3>

<p><strong>Abstract</strong></p>

<blockquote>
<p>Some programming language theorists would have us believe that the one true path to working systems lies in powerful and expressive type systems which allow us to encode rich constraints into programs at the time they are created. If these academic computer scientists would get out more, they would soon discover an increasing incidence of software developed in languages such a Python, Ruby and Clojure which use dynamic, albeit strong, type systems. They would probably be surprised to find that much of this software—in spite of their well-founded type-theoretic hubris—actually works, and is indeed reliable out of all proportion to their expectations.This talk—given by an experienced polyglot programmer who once implemented Hindley Milner static type inference for “fun”, but who now builds large and successful systems in Python—explores the disconnect between the dire outcomes predicted by advocates of static typing versus the near absence of type errors in real world systems built with dynamic languages: Does diligent unit testing more than make up for the lack of static typing?  Does the nature of the type system have only a low-order effect on reliability compared to the functional or imperative programming paradigm in use?  How often is the dynamism of the type system used anyway? How much type information can JITs exploit at runtime? Does the unwarranted success of dynamically typed languages get up the nose of people who write Haskell?</p>
</blockquote>

<p><strong>Summary</strong></p>

<p>The speaker used data from Github to determine that approximately 2.7% of Python bugs are type errors. Python&#39;s <code>TypeError</code>, <code>AttributeError</code>, and <code>NameError</code> were classified as type errors. The speaker rounded 2.7% down to 2% and claimed that 2% of errors were type related. The speaker mentioned that on a commercial codebase he worked with, 1% of errors were type related, but that could be rounded down from anything less than 2%. The speaker mentioned looking at the equivalent errors in Ruby, Clojure, and other dynamic languages, but didn&#39;t present any data on those other languages.</p>

<p>This data might be good but it&#39;s impossible to tell because there isn&#39;t enough information about the methodology. Something this has going for is that the number is in the right ballpark, compared to the made up number we got when compared the bug rate from Code Complete to the number of bugs found by Farrer. Possibly interesting, but thin.</p>

<h3 id="summary-of-summaries">Summary of summaries</h3>

<p>This isn&#39;t an exhaustive list. For example, I haven&#39;t covered “An Empirical Comparison of Static and Dynamic Type Systems on API Usage in the Presence of an IDE: Java vs. Groovy with Eclipse”, and “Do developers benefit from generic types?: an empirical comparison of generic and raw types in java” because they didn&#39;t seem to add much to what we&#39;ve already seen.</p>

<p>I didn&#39;t cover a number of older studies that are in the related work section of almost all the listed studies both because the older studies often cover points that aren&#39;t really up for debate anymore and also because the experimental design in a lot of those older papers leaves something to be desired. Feel free to <a href="https://twitter.com/danluu">ping me</a> if there&#39;s something you think should be added to the list.</p>

<p>Not only is this list not exhaustive, it&#39;s not objective and unbiased. If you read the studies, you can get a pretty good handle on how the studies are biased. However, I can&#39;t provide enough information for you to decide for yourself how the studies are biased without reproducing most of the text of the papers, so you&#39;re left with my interpretation of things, filtered through my own biases. That can&#39;t be helped, but I can at least explain my biases so you can discount my summaries appropriately.</p>

<p>I like types. I find ML-like languages really pleasant to program in, and if I were king of the world, we&#39;d all use F# as our default managed language. The situation with unmanaged languages is a bit messier. I certainly prefer C++ to C because std::unique_ptr and friends make C++ feel a lot safer than C. I suspect I might prefer Rust once it&#39;s more stable. But while I like languages with expressive type systems, I haven&#39;t noticed that they make me more productive or less bug prone.</p>



<p>Now that you know what my biases are, let me give you my interpretation of the studies. Of the controlled experiments, only three show an effect large enough to have any practical significance. The Prechelt study comparing C, C++, Java, Perl, Python, Rexx, and Tcl; the Endrikat study comparing Java and Dart; and Cooley&#39;s experiment with VHDL and Verilog. Unfortunately, they all have issues that make it hard to draw a really strong conclusion.</p>

<p>In the Prechelt study, the populations were different between dynamic and typed languages, and the conditions for the tasks were also different. There was a follow-up study that illustrated the issue by inviting Lispers to come up with their own solutions to the problem, which involved comparing folks like <a href="http://wry.me/~darius/hacks/lisp-study.scm">Darius Bacon</a> to random undergrads. A follow-up to the follow-up literally involves <a href="http://norvig.com/java-lisp.html">comparing code from Peter Norvig</a> to code from random college students.</p>

<p>In the Endrikat study, they specifically picked a task where they thought static typing would make a difference, and they drew their subjects from a population where everyone had taken classes using the statically typed language. They don&#39;t comment on whether or not students had experience in the dynamically typed language, but it seems safe to assume that most or all had less experience in the dynamically typed language.</p>

<p>Cooley&#39;s experiment was one of the few that drew people from a non-student population, which is great. But, as with all of the other experiments, the task was a trivial toy task. While it seems damning that none of the VHDL (static language) participants were able to complete the task on time, it is extremely unusual to want to finish a hardware design in 1.5 hours anywhere outside of a school project. You might argue that a large task can be broken down into many smaller tasks, but a plausible counterargument is that there are fixed costs using VHDL that can be amortized across many tasks.</p>

<p>As for the rest of the experiments, the main takeaway I have from them is that, under the specific set of circumstances described in the studies, any effect, if it exists at all, is small.</p>

<p>Moving on to the case studies, the two bug finding case studies make for interesting reading, but they don&#39;t really make a case for or against types. One shows that transcribing Python programs to Haskell will find a non-zero number of bugs of unknown severity that might not be found through unit testing that&#39;s line-coverage oriented. The pair of Erlang papers shows that you can find some bugs that would be difficult to find through any sort of testing, some of which are severe, using static analysis.</p>

<p>As a user, I find it convenient when my compiler gives me an error before I run separate static analysis tools, but that&#39;s minor, perhaps even smaller than the effect size of the controlled studies listed above.</p>

<p>I found the 0install case study (that compared various languages to Python and eventually settled on Ocaml) to be one of the more interesting things I ran across, but it&#39;s the kind of subjective thing that everyone will interpret differently, which you can see by looking.</p>

<p>This fits with the impression I have (in my little corner of the world, ACL2, Isabelle/HOL, and PVS are the most commonly used provers, and it makes sense that people would prefer more automation when solving problems in industry), but that&#39;s also subjective.</p>

<p>And then there are the studies that mine data from existing projects. Unfortunately, I couldn&#39;t find anybody who did anything to determine causation (e.g., <a href="http://www.amazon.com/gp/product/0691120358/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=0691120358&amp;linkCode=as2&amp;tag=abroaview-20&amp;linkId=XGRCRGSCGXSGNMJG">find an appropriate instrumental variable</a>), so they just measure correlations. Some of the correlations are unexpected, but there isn&#39;t enough information to determine why. The lack of any causal instrument doesn&#39;t stop people like Ray et al. from making strong, unsupported, claims.</p>

<p>The only data mining study that presents data that&#39;s potentially interesting without further exploration is Smallshire&#39;s review of Python bugs, but there isn&#39;t enough information on the methodology to figure out what his study really means, and it&#39;s not clear why he hinted at looking at data for other languages without presenting the data.</p>

<p>Some notable omissions from the studies are comprehensive studies using experienced programmers, let alone studies that have large populations of &#34;good&#34; or &#34;bad&#34; programmers, looking at anything approaching a significant project (in places I&#39;ve worked, a three month project would be considered small, but that&#39;s multiple orders of magnitude larger than any project used in a controlled study),  using &#34;modern&#34; statically typed languages, using gradual/optional typing, using modern mainstream IDEs (like VS and Eclipse), using modern radical IDEs (like LightTable), using old school editors (like Emacs and vim), doing maintenance on a non-trivial codebase, doing maintenance with anything resembling a realistic environment, doing maintenance on a codebase you&#39;re already familiar with, etc.</p>

<p>If you look at the internet commentary on these studies, most of them are passed around to justify one viewpoint or another. The Prechelt study on dynamic vs. static, along with the follow-ups on Lisp are perennial favorites of dynamic language advocates, and github mining study has recently become trendy among functional programmers.</p>

<p><img src="https://danluu.com/images/empirical-pl/confirmation_bias.png" alt="A twitter thread! In the twitter thread, someone who actually read the study notes that other factors dominate language (which is explicitly pointed out by the authors of the study), which prompts someone else to respond &#39;Why don&#39;t you just read the paper? It explains all this. In the abstract, even.&#39;"/></p>

<p>Other than cherry picking studies to confirm a long-held position, the most common response I&#39;ve heard to these sorts of studies is that the effect isn&#39;t quantifiable by a controlled experiment. However, I&#39;ve yet to hear a specific reason that doesn&#39;t also apply to any other field that empirically measures human behavior. Compared to a lot of those fields, it&#39;s easy to run controlled experiments or do empirical studies. It&#39;s true that controlled studies only tell you something about a very limited set of circumstances, but the fix to that isn&#39;t to dismiss them, but to fund more studies. It&#39;s also true that it&#39;s tough to determine causation from ex-post empirical studies, but the solution isn&#39;t to ignore the data, but to do more sophisticated analysis. For example, <a href="https://www.amazon.com/gp/product/0691120358/ref=as_li_qf_asin_il_tl?ie=UTF8&amp;tag=abroaview-20&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=0691120358&amp;linkId=8f8c46b4c33ea48f53edcc3691f57d5a">econometric methods</a> are often able to make a case for causation with data that&#39;s messier than the data we&#39;ve looked at here.</p>

<p>The next most common response is that their viewpoint is still valid because their specific language or use case isn&#39;t covered. Maybe, but if the strongest statement you can make for your position is that there&#39;s no empirical evidence against the position, that&#39;s not much of a position.</p>

<p><small>
If you&#39;ve managed to read this entire thing without falling asleep, you might be interested in <a href="https://danluu.com/everything-is-broken/">my opinion on tests</a>.</small></p><small>

<h4 id="responses">Responses</h4>

<p>Here are the responses I&#39;ve gotten from people mentioned in this post. Robert Smallshire said &#34;Your review article is very good. Thanks for taking the time to put it together.&#34; On my comment about the F# &#34;mistake&#34; vs. trolling, his reply was &#34;Neither. That torque != energy is obviously solved by modeling quantities not dimensions. The point being that this modeling of quantities with types takes effort without necessarily delivering any value.&#34; Not having done much with units myself, I don&#39;t have an informed opinion on this, but my natural bias is to try to encode the information in types if at all possible.</p>

<p>Bartosz Milewski said <a href="https://twitter.com/BartoszMilewski/status/532651581813325824">&#34;Guilty as charged!&#34;</a>. Wow. Much Respect. But notice that, as of this update, The correction has been retweeted 1/25th as often as the original tweet. People want to believe there&#39;s evidence their position is superior. People don&#39;t want to believe the evidence is murky, or even possibly against them. Misinformation people want to believe spreads faster than information people don&#39;t want to believe.</p>

<p>On a related twitter conversation, Andreas Stefik said &#34;That is not true. It depends on which scientific question. Static vs. Dynamic is well studied.&#34;, &#34;Profound rebuttal. I had better retract my peer reviewed papers, given this new insight!&#34;, &#34;Take a look at the papers...&#34;, and &#34;This is a serious misrepresentation of our studies.&#34; I muted the guy since it didn&#39;t seem to be going anywhere, but it&#39;s possible there was a substantive response buried in some later tweet. It&#39;s pretty easy to take twitter comments out of context, so <a href="https://twitter.com/garybernhardt/status/531513870276231168">check out the thread yourself</a> if you&#39;re really curious.</p>

<p>I have a lot of respect for the folks who do these experiments, which is, unfortunately, not mutual. But the really unfortunate thing is that some of the people who do these experiments think that static v. dynamic is something that is, at present, &#34;well studied&#34;. There are plenty of equally difficult to study subfields in the social sciences that have multiple orders of magnitude more research going on, that are considered open problems, but at least some researchers already consider this to be well studied!</p>

<h4 id="acknowledgements">Acknowledgements</h4>

</small><p><small>Thanks to Leah Hanson, Joe Wilder, Robert David Grant, Jakub Wilk, Rich Loveland, Eirenarch, Edward Knight, and Evan Farrer for comments/corrections/discussion.
</small></p>

</div></div>
        </section>
    </article>
</main>
</body>
</html>
